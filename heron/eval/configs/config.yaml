wandb:
  log: True
  entity: "vision-language-leaderboard"
  project: "heron-leaderboard"
  run_name: "microsoft/Phi-3-vision-128k-instruct" 
  launch: false

# basic information
testmode: false
torch_dtype: "fp16" # {fp16, bf16, fp32}
api: false
model_artifact: null
processor_artifact: null
tokenizer_artifact: null

model:
  _target_: null
  pretrained_model_name_or_path:  "microsoft/Phi-3-vision-128k-instruct"
  automatic_adapter_generation: true # this is experimental. if true, the model adapter will be automatically generated based on the downloaded README.md.
  api_type: "claude-3-opus" # for automatic_adapter_generation, 'gpt-4' or 'claude-3-opus'
  skip_approval: false # if true, the approval process for using generated code will be skipped.
  torch_dtype: "float16"
  ignore_mismatched_sizes: true

processor:
  _target_: null
  pretrained_model_name_or_path: "your-model-name"
  
tokenizer: 
  _target_: null
  pretrained_model_name_or_path: null
  args:
    padding_side: "right"
    additional_special_tokens: '["▁▁"]'

datasets:
  llava_bench_in_the_wild_artifact_path: 'vision-language-leaderboard/heron-leaderboard/llava-bench-in-the-wild:v0'
  llava_bench_in_the_wild_reference_path: 'vision-language-leaderboard/heron-leaderboard/llava-bench-in-the-wild-reference:v0'
  japanese_heron_bench_artifact_path: 'vision-language-leaderboard/heron-leaderboard/japanese-heron-bench:v0'
  japanese_heron_bench_reference_path: 'vision-language-leaderboard/heron-leaderboard/heron-bench-reference:v0'

generation:
  args:
    max_length: 256
    do_sample: false
    temperature: 0.0
    eos_token_id_list: '[]'
    no_repeat_ngram_size: 2
